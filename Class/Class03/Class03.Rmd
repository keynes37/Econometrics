---
title: "Econometria I"
subtitle: "MCO"
author: "Carlos Yanes"
date: "Universidad del Norte </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["rutgers", "rutgers-fonts"]
    nature:
      beforeInit: "http://www.jvcasillas.com/ru_xaringan/js/ru_xaringan.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false
---
name: xaringan-title
class: left, middle


# Econometr√≠a I
<br>
## M√≠nimos Cuadrados Ordinarios

<br>
<br>
<img src="images/lognig.png" width="280" />

### Carlos A. Yanes | Departamento de Econom√≠a | `r Sys.Date()`

---

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel)

library(tidyverse)
library(babynames)
library(fontawesome) # from github: https://github.com/rstudio/fontawesome
library(DiagrammeR)
library(fpp2)

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_empty + theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)

theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)

```

```{R, colors, include = F}
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
blue_ty <- "#3b29e3"
```

background-size: 100%
background-image: url(https://media.giphy.com/media/VP2F9tqaCmUarK7GrU/giphy.gif)

???

Image test. Taken from gyfty.

---
class: middle, inverse
.left-column[

# `r emo::ji("confused")`

]

.right-column[
# Preguntas de la sesi√≥n anterior?
]
---
# Preliminar

--

La √∫ltima vez:

--

1. Hasta el momento hemos hablado de estadisticas.

1. Hoy hablaremos mejor de las condiciones **MELI** de un estimador

1. Vamos a mirar algunas lineas de c√≥digo en **.blue[R]** 

1. Para eso pensaremos en eventos con .RUred[muestras de datos].

---
class: title-slide-section-red, middle

# Modelo Poblacional vs Muestral 

<br>
<img src="images/lognig.png" width="380" />

---
# Modelo Poblacional vs Muestral 

--

Podemos tener un modelo <span style="font-size:larger;">**Poblacional**</span>

--

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$

--

Y uno <span style="font-size:larger;">.black[Muestral]</span>  de la siguiente forma

--

$$ y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i $$

--

Un **modelo de regresi√≥n** produce un estimador por cada observaci√≥n

--

$$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$

--

El cual nos dar√° el _mejor-ajuste_ lineal a partir de nuestros datos.

---
class: title-slide-section-grey, middle

# Poblaci√≥n *vs.* Muestra

<br>
<img src="images/lognig.png" width="380" />


---
layout: true

# Poblaci√≥n *vs.* Muestra

**Pregunta:** Por qu√© nos preocupa eso de la *poblaci√≥n vs. muestra*?

---

--

```{R, gen dataset, include = F, cache = T}
# Poblacion y muestra
n_p <- 100
n_s <- 30
# Semilla
set.seed(12468)
# Generar datos
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regresiones
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulaci√≥n
set.seed(12468)
sim_df <- mclapply(mc.cores = 10, X = 1:1e4, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{R, pop1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_empty
```

.center[**Poblaci√≥n**]

]

--

.pull-right[

```{R, scatter1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
geom_point(color = "darkslategray", size = 6) +
theme_empty
```

.center[**Relaci√≥n Poblacional**]

$$ y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i $$

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$


]

---

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 1:** 30 individuos de forma aleatoria]

]

--

.pull-right[

```{R, sample1 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relaci√≥n Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relaci√≥n Muestral**
<br>
$\hat{y}_i = `r round(lm1$coefficients[1], 2)` + `r round(lm1$coefficients[2], 2)` x_i$

]

]

---
count: false

.pull-left[

```{R, sample2, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 2:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample2 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relaci√≥n Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relaci√≥n Muestral**
<br>
$\hat{y}_i = `r round(lm2$coefficients[1], 2)` + `r round(lm2$coefficients[2], 2)` x_i$

]

]
---
count: false

.pull-left[

```{R, sample3, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 3:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample3 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relaci√≥n Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relaci√≥n Muestral**
<br>
$\hat{y}_i = `r round(lm3$coefficients[1], 2)` + `r round(lm3$coefficients[2], 2)` x_i$

]

]

---
layout: false
class: clear, middle

Podemos repetir esto **10,000 veces**.

(Este ejercicio se llama simulaci√≥n de (Monte Carlo) )

---
layout: false
# Poblaci√≥n *vs.* Muestra

```{R, simulation scatter, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01) +
geom_point(data = pop_df, aes(x = x, y = y), size = 3, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 1.5
) +
theme_empty
```

---
layout: true
# Poblaci√≥n *vs.* Muestra

**Pregunta:** Por qu√© nos preocupa eso de la *poblaci√≥n vs. muestra*?

---

.pull-left[
```{R, simulation scatter2, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01, size = 1) +
geom_point(data = pop_df, aes(x = x, y = y), size = 6, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
theme_empty
```
]

.pull-right[

- En **promedio**, nuestras l√≠neas de regresi√≥n coinciden con la l√≠nea de la poblaci√≥n de forma correcta.

- Sin embargo, **Lineas individuales** (muestras) pueden fallar.

- Las diferencias entre las muestras individuales y de la poblaci√≥n generan **incertidumbre** para el econometrista.

]

---

--

**Respuesta:** La incertidumbre es importante.

--

- Se esta `interesado` en **describir** y **evaluar** la relaci√≥n entre una variable determinada (denominada _explicada_ o _dependiente_) y una o m√°s otras variables (com√∫nmente llamadas variables 
_explicativas_ o independientes).

--

- Estableceremos como la variable _dependiente_ por $(y)$, mientras que las `independientes` por $x_{1}, x_{2}, x_{k}$. 

--

`r fa('magic')` Si $k=1$, solo hay una de las $k$-variables, por ende se estima una regresi√≥n `simple`.

--

`r fa('magic')` Si $k>1$, hay m√°s de las $k$-variables, tenemos entonces un modelo de regresi√≥n `m√∫ltiple`.

---
layout: false
class: title-slide-section-grey, middle

# Modelos

<br>
<img src="images/lognig.png" width="380" />


---
# Modelos `r emo::ji("bullseye")`

--

`r fa('barcode')` Un ejemplo de modelos de regresi√≥n
$$\begin{equation}
y= \text{Salario por horas} \\ 
x = \text{A√±os de educaci√≥n}
\end{equation}$$

--

- `Objetivo`: **Determinar la relaci√≥n entre $(y)$ (Salario) y $(x)$ (a√±os de educaci√≥n)**.

--

- Un modelo mas general y con m√∫ltiples variables, como es el caso de los **Salario en funci√≥n de la educaci√≥n y otras car√°cteristicas**:

--

$$\begin{aligned}
y &= \text{Salario por horas} \\ 
x_{1} &= \text{A√±os de educaci√≥n} \\ 
x_{2} &= \text{Edad}\\ 
x_{3} &= \text{Experiencia}
\end{aligned}$$

--

- `Objetivo`: **Determinar la relaci√≥n entre $(y)$ (salario ) y $(x's)$ (a√±os de educaci√≥n, edad y la experiencia)**.

---
# Modelos `r emo::ji("bullseye")`

--

**Hay varios** `objetivos` en estudiar este tipo de _relaciones_

--

-  Analizar los **efectos** de pol√≠ticas que envuelven cambiar los $x's$ individuales.

--

- Pronosticar **el valor** de $y$ para un determinado conjunto de $x's$.

--

- Examinar si alguno de los $x's$ tiene un **efecto** significativo en $y$.

--

`r fa('flask')` <span style="color:blue"> **Comparaciones estad√≠sticas y deterministicas** </span>

--

- En las relaciones **estad√≠sticas** entre variables tratamos esencialmente con variables aleatorias (variables que tienen distribuciones de probabilidad).

--

- En la dependencia funcional o **determin√≠stica** tambi√©n manejamos variables, pero no son aleatorias (ejemplo: leyes f√≠sica).

---
class: title-slide-section-red, middle

# Regresi√≥n vs. Causalidad 

<br>
<img src="images/lognig.png" width="380" />


---
# Regresi√≥n vs. Causalidad `r emo::ji("mountain")`

--

`r fa('play-circle')` A pesar de que el **an√°lisis de regresi√≥n** tiene que ver con la _dependencia_ de una `variable` respecto a otras `variables`, esto no implica causalidad necesariamente.

--

`r fa('play-circle')` Para aducir **causalidad** se debe acudir a consideraciones a priori o te√≥ricas.

--

`r fa('play-circle')` **Ejemplo**: Un estudio de la  dependencia existente entre el producto de una cosecha y la temperatura, lluvia, cantidad de sol y fertilizantes.

--

> No hay una relaci√≥n estad√≠stica para suponer que la lluvia no depende del producto de la cosecha. El hecho que el producto de la cosecha se considere como dependiente de la lluvia (entre otros) es debido a otras consideraciones, como por ejemplo el _sentido com√∫n_.

---
# Regresi√≥n vs. Causalidad `r emo::ji("mountain")`

--
#### Estructura de un modelo `r emo::ji("jockey")`

--

(X,Y) son dos variables _aleatorias_, que representan a alguna poblaci√≥n, y estamos interesados en `explicar Y en t√©rminos de X` o en "estudiar como _varia_ Y con cambios en X".

--

$$\begin{aligned}
     \underbrace{Y}_{\text{Variable dependiente}} = \underbrace{\beta_{0}}_{\text{Par√°metro intercepto}}+
\underbrace{\beta_{1}}_{\text{Par√°metro pendiente}} \underbrace{X}_{\text{Variable independiente}} +
\underbrace{\mu}_{\text{T√©rmino de error}} 
  \end{aligned}$$

--

- El par√°metro $\mu$ es una variable aleatoria _no observable_ que toma valores positivos o negativos, en t√©rminos generales representa _otros_ factores de X que afectan a Y.

--

- La(s) variable(s) $X$ tiene un efecto lineal en $Y\;\Rightarrow \quad \triangle Y = \beta_{1} \triangle X$ si y solo si $\; \triangle \mu = 0$.

---
class: title-slide-section-red, middle

# Otro ejemplo

<br>
<img src="images/lognig.png" width="380" />
---
# Piense en lo siguiente üõë

--

`r fa('barcode')` La directora de escuelas primarias de una localidad de Barranquilla quiere responder la siguiente pregunta: 

--

- Si se reduce el **tama√±o promedio** de las clases en dos (2) estudiantes, `¬øcu√°l es el efecto en las calificaciones obtenidas por el resto del curso en un examen de cierta asignatura?`

--

> Una respuesta precisa a la _pregunta_ exige una cuantificaci√≥n de las _variaciones_: si la directora var√≠a el n√∫mero de alumnos por clase en cierta cantidad, `¬øqu√© variaci√≥n esperar√≠a que sucediese sobre las puntuaciones de los ex√°menes?` 

--

- Una posible respuesta es:

--

$$\beta_{i}\equiv\beta_{\text{Tama√±o clase}}= \frac{\text{Variaci√≥n Calif Examen}}{\text{Variaci√≥n Tama√±o Clase}} = \frac{\triangle \text{Calificaci√≥n Examen}}{\triangle \text{Tama√±o Clase}}$$ 
---
# Piense en lo siguiente üõë

--

- Se podr√≠a `responder` a la pregunta real de la directora reorganizando la ecuaci√≥n:
$$\triangle \text{Calificaci√≥n Examen} = \beta_{\text{Tama√±o Clase}} \times \triangle \text{Tama√±o Clase}$$
--


- Si por alguna manera $\beta_{\text{Tama√±o Clase}}=-0.6$, una reducci√≥n en dos alumnos da como `variaci√≥n` de las calificaciones esperadas de $(-0.6) \times (-2) =1.2$.

--

La **l√≠nea** recta que relaciona las _calificaciones_ y el _Tama√±o de la clase_ puede escribirse como:

$$\text{Calificaci√≥n examen}= \beta_{0} + \beta_{i} \times \text{Tama√±o Clase}$$

--

Recuerde que $\beta_{i}$ es el .blue[par√°metro] del tama√±o de la clase

--

>Esta **ecuaci√≥n** no se cumple con exactitud para todas las **localidades**. Una versi√≥n de esta _relaci√≥n lineal_ que se cumpliera en cada distrito deber√≠a incorporar otros factores que pueden influir en las calificaciones, incluyendo las caracter√≠sticas √∫nicas de cada uno de los distritos (ejemplos: calidad maestros, caracter√≠sticas alumnos, fortuna estudiantes el d√≠a del examen, etc.)

---
# Piense en lo siguiente üõë

--

`r fa("wrench", fill="red")` Suponga que quisi√©ramos `predecir` la nota del examen de matem√°ticas dado *cierto tama√±o de la clase*, entonces tendremos:

--

$$\text{Calificaci√≥n examen}= 27 -0.6 \times \text{Tama√±o Clase} + \mu_i$$
--

Si colocamos como tama√±o de clase el n√∫mero de 40 estudiantes, entonces vamos a tener en promedio como resultado de nota 3.0. Observe que si el tama√±o de la clase fuera ahora de 38. La **calificaci√≥n** entonces estar√≠a rondando una nota de 4.2.  

---
# Piense en lo siguiente üõë

--

#### Un modelo completo üçÑ

--

Es de pensar, que entonces un modelo m√°s `completo` es:

--

$$\text{Calificaci√≥n examen}= \beta_{0} + \beta_{\text{Tama√±o Clase}} \times \text{Tama√±o Clase} + \text{Otros factores}$$ 

--

`r fa('minus-square')` Siempre es bueno tener en cuenta los supuestos del **Modelo de regresi√≥n**

--

Estos son:

--

`r fa("adn", fill=red)` Sea $\left \{ (X_{i},Y_{i}: \; i= 1,2,3,\dots,n  ) \right\}$ una muestra _aleatoria_ de tama√±o $n$ de la poblaci√≥n:

--

$$Y_{i}= \beta_{0}+\beta_{1} X_{i}+ \mu_{i} \; i=1,2,3,\dots,n$$

--

Nuestro objetivo es tener estimado los **par√°metros** desconocidos $\beta_{0}$ y $\beta_{1}$ dadas las $n$ observaciones de $(X,Y)$. _Para esto, tenemos algunos supuestos sobre $\mu$_.

---
# Piense en lo siguiente üõë

--

.pull-left[
----

```{c1, warning=FALSE, message=FALSE, eval=FALSE, echo=TRUE}
library(wooldridge)
library(tidyverse)
data("ceosal1")

mi_modelo<-lm(salary~roe, ceosal1)
summary(mi_modelo)
```
----

.center[Qu√© interpretaci√≥n tiene lo anterior?
.hi-red[$$\hat{salary}=963.19+18501 \;roe$$]]
- Lo que si el rendimiento del **capital** es cero $roe=0$, el sueldo (intercepto), la parte de 963.191 es el salario promedio que recibe el gerente. Ya que el salario se mide en miles esto se interpreta as√≠ en t√©rminos de las unidades de $\hat{y}$.
]

.pull-right[
```{r, warning=FALSE, message=FALSE, eval=TRUE, echo=FALSE}
library(wooldridge)
library(tidyverse)
data("ceosal1")

mi_modelo<-lm(salary~roe, ceosal1)
summary(mi_modelo)
```


- Lo que tenemos, el **cambio** que se predice para el sueldo en funci√≥n del cambio en el `roe` se expresa tal que:

$$\vartriangle \hat{salary}= 18,501 (\vartriangle roe)$$

- Esto indica que cuando el rendimiento del capital de la empresa aumenta en un punto porcentual, $roe=1$, podemos predecir que el sueldo del gerente varie en aproximadamente $18.500 para un gerente, manteniendo todo lo demas constante

]


---
class: title-slide-section-red, middle

# Los supuestos de residuo y la estimaci√≥n

<br>
<img src="images/lognig.png" width="380" />
---
# Supuestos de los residuos $\mu$ o $\epsilon$

--

1. **Media cero**: $E(\mu_{i})=0 \; \forall i$.

--

2. **Varianza com√∫n**: $var(\mu_{i})=\sigma^{2} \; \forall i$.

--

3. **Independencia (no correlaci√≥n serial)**: $\mu_{i}$ y $\mu_{j}$ son independientes para todo $i\neq j$. Dado $(X_{i})$, las desviaciones de dos valores cualquiera de Y de su media no muestran valores _sistem√°ticos_.

--

4. **Independencia** de $X_{j}: \mu_{i} \; y \; X_{j}$ son independientes para todo i y j.  Intuitivamente, `si no se cumple` entonces es dif√≠cil aislar la influencia de X y $\mu$ sobre Y.

--

5. **Normalidad**: $\mu_{i}$ est√° normalmente distribuida para todo i.

---
class: title-slide-section-red, middle

# Regresi√≥n lineal

<br>
<img src="images/lognig.png" width="380" />



---
# El estimador üö©

--

Podemos estimar la regresi√≥n en .mono[R] (`lm(y ~ x, my_data)`). Pero esas estimaciones de donde provienen?

--

Repasemos

> $$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$$

--

El cual nos da *mejor-ajuste* lineal de nuestros datos.
Pero que significa eso de "Linea de mejor ajuste"?

--

- En (econometr√≠a), *mejor-ajuste* significa que la _linea_ de los datos minimiza la suma del error al cuadrado (SSE):

.center[
$\text{SSE} = \sum_{i = 1}^{n} e_i^2\quad$ donde $\quad e_i = y_i - \hat{y}_i$
]

--

- M√≠nimos  **cuadrados ordinarios** (**MCO**) minimiza la suma de los errores al cuadrado.

--

- Basado en una serie de supuestos (en su mayor√≠a aceptables), MCO:

--

  - Es insesgado (y consistente)
  - Es el *mejor* (m√≠nima varianza) estimador lineal insesgado (MELI)
  
---
Tomemos como referencia la base de datos poblacional. $\color{#ffffff}{\bigg|}$

--

```{R, ols vs lines 1, echo = F, dev = "svg", fig.height = 6}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier linea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$ $\color{#ffffff}{\bigg|}$

```{R, vs lines 2, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier linea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular sus errores: $e_i = y_i - \hat{y}_i$ $\color{#ffffff}{\bigg|}$

```{R, ols vs lines 3, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier linea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular sus errores: $e_i = y_i - \hat{y}_i$ $\color{#ffffff}{\bigg|}$

```{R, ols vs lines 4, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier linea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular sus errores: $e_i = y_i - \hat{y}_i$ $\color{#ffffff}{\bigg|}$

```{R, ols vs lines 5, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

SSE errores al cuadrado $\left(\sum e_i^2\right)$: los errores mas grandes seran mayormente penalizados. $\color{#ffffff}{\bigg|}$

```{R, ols vs lines 6, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
count: false

La estimaci√≥n MCO busca tener un $\hat{\beta}_0$ y un $\hat{\beta}_1$ que minimiza a SSE. $\color{#ffffff}{\bigg|}$

```{R, ols vs lines 7, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```


---
layout: true
# El estimador üö©

---

### Formalmente

En el modelo de regresi√≥n simple, el estimador MCO vendr√° a ser obtenido mediante $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimiza la suma de los residuos al cuadrado (SSE), _p.e._,

--

$$\min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SSE}$$

--

Pero ya sabemos que $\text{SSE} = \sum_i e_i^2$. Ahora definimos a los residuos  $e_i$ y el valor predicho de la .black[dependiente] $\hat{y}$.

--

$$
\begin{aligned}
  e_i^2 &= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}
$$

--

**Recuerde:** Minimizar una funci√≥n multivariada requiere (**1**) que la primera derivada (La condici√≥n de *1.super[er]-orden*) y (**2**) condici√≥n de segundo-orden o (concavidad).

---
Nos estamos acercando. Tenemos que **minimizar la SSE**. Hemos mostrado c√≥mo se relaciona el SSE con nuestra muestra (nuestros datos: $x$ e $y$) y nuestras estimaciones (_p.e._, $\hat{\beta}_0$ y $\hat{\beta}_1$).

--

$$\text{SSE} = \sum_i e_i^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right)$$

--

Para las condiciones de primer orden de minimizaci√≥n, tomamos ahora las primeras derivadas de SSE con respecto a $\hat{\beta}_0$ y $\hat{\beta}_1$.

--

$$
\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} &= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}
$$

--

donde $\overline{x} = \frac{\sum x_i}{n}$ y $\overline{y} = \frac{\sum y_i}{n}$ son las medias muestrales de $x$ e $y$ (tama√±o $n$).

---
Las condiciones de primer orden establecen que las derivadas son iguales a cero, por lo que:

--

$$\dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0$$

--

Lo cual implica

--

$$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$
--

`r fa("arrow-alt-circle-right", fill="red")` Este .black[estimador] viene a ser la diferencia entre los promedios de nuestras variables dependientes e independientes teniendo presente el efecto de $\hat{\beta}_1$.

--

Ahora solo nos falta por hallar $\hat{\beta}_1$.

---
Hay que tomar la derivada de SSE con respecto a $\hat{\beta}_1$

--

$$
\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} &= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}
$$

--

todo igual a cero (condici√≥n de primer-orden, de nuevo)

--

$$\dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0$$

--

y sustituimos $\hat{\beta}_0$, _p.e._, $\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$. As√≠,

--

$$
 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
$$

---
De lo anterior

--

$$ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$
--

a multiplicar

--

$$2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0$$

--

$$\implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x}$$

--

$$ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

---
Hecho!

--

Ahora tenemos estimadores OLS (encantadores) para la pendiente

--

$$\hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2}$$
--

Para el intercepto o $\beta_{0}$

$$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$

--

Y ahora **ya saben de d√≥nde** viene la parte de *m√≠nimos cuadrados* de MCO.

---
layout: false
class: title-slide-section-red, middle

# Otras condiciones 

<br>
<img src="images/lognig.png" width="380" />


---
# Propiedades de los estimadores de MCO ‚ö†

--

1. Los estimadores deben ser **lineales** sumado a las perturbaciones.

--

1. Nuestras variables .black[X] son exogenas, p.e: $E[\mu|X]=0$

--

1. La relaci√≥n entre las variables explicativas .black[X] es inexistente, de lo contrario padecera de *multicolinealidad*.

--

1. La perturbaci√≥n tiene media cero $E[\mu]=0$ y varianza constante $(\sigma^2)$, su distribuci√≥n debe ser independiente e id√©nticamente distribuida.

---
# Propiedades de los estimadores de MCO ‚ö†

--

$$E[\mu|X]=0$$

--

Es una de las propiedades mas restrictivas. El cumplimiento de los supuestos 1-3 nos garantiza .black[insesgadez] en los estimadores. Ya se hace necesario tener 4 para decir que entonces es .black[m√≠nima varianza].

--

_Un ejemplo_ 

--

$$E[\mu|X=10]=0 \quad \text{de igual manera}\quad E[\mu|X=100]=0$$

--

Incluso con variables cualitativas, la condici√≥n debe mantenerse, esto es:

--

$$E[\mu|X=mujer]=0 \quad \text{de igual manera}\quad E[\mu|X=hombre]=0$$
---
class: title-slide-section-grey, middle

# Exogeneidad estricta

<br>
<img src="images/lognig.png" width="380" />

---
# Exogeneidad estricta

```{R, conditional_expectation_setup, include = F, cache = T}
# Setup ----------------------------------------------------------------------------------
  # Options
  options(stringsAsFactors = F)
  # Packages
  library(pacman)
  p_load(ggridges)
# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(12345)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n) + 0.5 * x
  ) %>% mutate(x = round(x))
# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    theme_pander()
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    theme_pander()
# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("u") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("u") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
```
---
Esa validez es, _p.e._, $\mathop{E}\left[ u \mid X \right] = 0$

```{R, ex_good_exog, echo = F, dev = "svg"}
cond_good
```
---
Esa validez no se da cuando, _p.e._, $\mathop{E}\left[ u \mid X \right] \neq 0$

```{R, ex_bad_exog, echo = F, dev = "svg"}
cond_bad
```

---
class: title-slide-section-red, middle

# Estimaci√≥n en `r fa("r-project", fill = "steelblue")` 

<br>
<img src="images/lognig.png" width="380" />

---
# Estimaci√≥n en `r fa("r-project", fill = "steelblue")` 

--

### La opci√≥n por default es: `lm()`

--

La forma de estimaci√≥n en .black[R] para usar como `base`.super[‚Ä†]  para estimar los modelos de Regresi√≥n .RUred[l]ineal es `lm()`.

--

.footnote[‚Ä† `base` es el formato por default del algoritmo <br> .RUred[‚Ä†‚Ä†] Puede remover el intercepto solo colocando `-1` dentro de la formula, _p.e._, `lm(y ~ -1 + x)`.]

--

Puede hacerlo directamente

--

`lm(y ~ x)`

--

- Esto estima $y_i = \beta_0 + \beta_1 x_i + u_i$ (.black[R] lo hace autom√°ticamente incluyendo el t√©rmino del .blue[intercepto]).super[.RUred[‚Ä†‚Ä†]]

--

- Los datos se vinculan como objetos columna `(y)` (dependiente) y ademas  `(x)` (independientes).

--

`lm(y ~ x, data = bd_Dane)`

--

- Estimamos $y_i = \beta_0 + \beta_1 x_i + u_i$

--

- Usando las columnas de `y` ademas de `x` del objeto `bd_Dane`.

---
# Estimaci√≥n en `r fa("r-project", fill = "steelblue")` 

--

### Ademas de `lm()`

--

Si necesita incluir mas variables? Pues... f√°cil

--

`lm(y ~ x1 + x2 + x3, data = alguna_bd)`
- Donde estima $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + u_i$
- La referencia de `alguna_bd` es para estipular la base de datos a usar.

---
# Estimaci√≥n en `r fa("r-project", fill = "steelblue")` 

--

### Algo mas de `lm()`

--

Si requiere transformar/interactuar con variables? Tambi√©n es f√°cil: debe usar para eso `I()`.

--

`lm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2), data = bd_Dane)`
- Esto estima $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{1i}^2 + \beta_4 x_{2i}^2 + \beta_5 x_{1i} x_{2i} + u_i$
- Utilizando las variables del objecto `bd_Dane` (donde est√°n los datos)
- o se crean/generan v√≠a `I()`

--

.grey[Nota:] Los siguientes *ejemplos* son equivalentes:

--

- `lm(y ~ x1 + x2 + I(x1*x2))`
- `lm(y ~ x1 + x2 + x1:x2)`
- `lm(y ~ x1*x2)`


---
name: transformations
# Estimaci√≥n en `r fa("r-project", fill = "steelblue")` 

--

### Transformando variables con `lm()`

--

Observe lo siguiente:

--

`lm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2), data = bd_Dane)`

--

No necesitamos crear $x_1^2$, $x_2^2$, ademas de $x_1\times x_2$ en el conjunto de datos.

--

El programa de .mono[R] hace el calculo por nosotros (siempre y cuando `x1` y `x2` existan en la base de datos).

--

Cualquier **transformaci√≥n** que quiera hace es posible

--

- Transformaci√≥n Matem√°tica/estad√≠stica: `I(x^2)`, `I(x/3)`, `I((x - mean(x))/sd(x))`
- Log/exponenenciales : `log(x)`, `exp(x)`
- Indicadores: `I(x < 100)`, `I(x == "Barranquilla")`


---
# Bibliograf√≠a

`r fa('book')` √Ålvarez, R. A. R., Calvo, J. A. P., Torrado, C. A. M., & Mondrag√≥n, J. A. U. (2013). *Fundamentos de econometr√≠a intermedia: teor√≠a y aplicaciones*. Universidad de los Andes.

`r fa('book')` Stock, J. H., Watson, M. W., & Larri√≥n, R. S. (2012). *Introducci√≥n a la Econometr√≠a*.

`r fa('book')` Wooldridge, J. M. (2015). *Introductory econometrics: A modern approach*. Cengage learning.

---
class: title-slide-final, middle

# Gracias por su atenci√≥n!

## Alguna pregunta adicional?

### Carlos Andres Yanes Guerra
`r fa("envelope", fill="red")` cayanes@uninorte.edu.co
`r fa("twitter", fill="cyan")` keynes37
